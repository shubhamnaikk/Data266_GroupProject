from fastapi import FastAPI, HTTPException, Body, Query
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
import os, json, hashlib, time
import psycopg
from psycopg.rows import dict_row

# control-plane DSNs (same as before)
APP_RO_DSN   = os.getenv("APP_RO_DSN")
LOADER_RW_DSN= os.getenv("LOADER_RW_DSN")

# Connectors
from connectors.connector_base import single_statement_select_only
from connectors.postgres_external import PostgresExternal
from connectors.mysql import MySQLConnector
from connectors.snowflake import SnowflakeConnector

def get_cp_conn(write=False):
    dsn = LOADER_RW_DSN if write else APP_RO_DSN
    return psycopg.connect(dsn, autocommit=True)

def load_connection(conn_id:int)->Dict[str,Any]:
    with get_cp_conn(False) as cp, cp.cursor(row_factory=dict_row) as cur:
        cur.execute("SELECT * FROM connections WHERE id=%s",(conn_id,))
        row = cur.fetchone()
        if not row:
            raise HTTPException(404, f"connection {conn_id} not found")
        return row

def build_connector(rec:Dict[str,Any]):
    driver = rec["driver"]
    dsn = rec["dsn"]
    if driver=="postgres": return PostgresExternal(dsn)
    if driver=="mysql":    return MySQLConnector(dsn)
    if driver=="snowflake":return SnowflakeConnector(dsn)
    raise HTTPException(400, f"unsupported driver: {driver}")

app = FastAPI(title="DBLens MVP â€“ Plug & Play")

# -------------------- Models --------------------
class NewConnection(BaseModel):
    name: str
    driver: str  # postgres | mysql | snowflake
    dsn: Optional[str] = None
    secret_ref: Optional[str] = None

class FromURL(BaseModel):
    url: str
    table: str
    format: Optional[str] = "auto"
    if_exists: Optional[str] = "fail"

class SQLBody(BaseModel):
    sql: str
    conn_id: Optional[int] = None
    limit: Optional[int] = None
    question: Optional[str] = None

# -------------------- Connections --------------------
@app.post("/connections")
def add_connection(body: NewConnection):
    if body.driver not in ("postgres","mysql","snowflake"):
        raise HTTPException(400, "driver must be one of postgres|mysql|snowflake")
    with get_cp_conn(True) as cp, cp.cursor(row_factory=dict_row) as cur:
        cur.execute("""
            INSERT INTO connections(name,driver,dsn,secret_ref,features_json,read_only_verified,created_at,last_tested_at)
            VALUES(%s,%s,%s,%s,%s,false,now(),NULL)
            RETURNING id
        """,(body.name, body.driver, body.dsn, body.secret_ref, json.dumps({})))
        cid = cur.fetchone()["id"]
        return {"ok": True, "id": cid}

@app.get("/connections")
def list_connections():
    with get_cp_conn(False) as cp, cp.cursor(row_factory=dict_row) as cur:
        cur.execute("SELECT id,name,driver,read_only_verified,features_json,created_at,last_tested_at FROM connections ORDER BY id")
        return {"connections": cur.fetchall()}

@app.post("/connections/test")
def test_connection(conn_id: int = Body(..., embed=True)):
    rec = load_connection(conn_id)
    try:
        conn = build_connector(rec)
        res = conn.test_connection()
        # try to verify read-only by attempting forbidden CREATE (expect failure)
        ro_ok = True
        try:
            conn.execute_readonly("CREATE TABLE should_fail(x int)")
            ro_ok = False
        except Exception:
            ro_ok = True
        with get_cp_conn(True) as cp, cp.cursor() as cur:
            cur.execute("UPDATE connections SET features_json=%s, read_only_verified=%s, last_tested_at=now() WHERE id=%s",
                        (json.dumps(res), ro_ok, conn_id))
        return {"ok": True, "features": res, "read_only_verified": ro_ok}
    except Exception as e:
        raise HTTPException(400, f"test failed: {e}")

# -------------------- Schema Cards --------------------
@app.get("/schema/cards")
def schema_cards(conn_id: Optional[int] = Query(None)):
    if conn_id:
        rec = load_connection(conn_id)
        conn = build_connector(rec)
        card = conn.introspect_schema(limit_samples=5)
        # cache (best-effort)
        with get_cp_conn(True) as cp, cp.cursor() as cur:
            for t in card.get("tables",[]):
                fqn = f'{t.get("schema","")}.{t.get("name","")}'
                cur.execute("""
                  INSERT INTO schema_card_cache(conn_id, table_fqn, columns_json, samples_json, refreshed_at)
                  VALUES (%s,%s,%s,%s,now())
                  ON CONFLICT(conn_id, table_fqn) DO UPDATE
                  SET columns_json=EXCLUDED.columns_json, samples_json=EXCLUDED.samples_json, refreshed_at=now()
                """,(conn_id, fqn, json.dumps(t.get("columns", [], default=_jd), default=_jd), json.dumps(t.get("samples", {}, default=_jd), default=_jd)))
        return {"SchemaCard": card}
    # fallback: existing local view (for backward compat)
    with get_cp_conn(False) as cp, cp.cursor(row_factory=dict_row) as cur:
        cur.execute("""
            SELECT table_schema as schema, table_name as name
            FROM information_schema.tables
            WHERE table_type='BASE TABLE' AND table_schema NOT IN ('pg_catalog','information_schema')
            ORDER BY 1,2
        """)
        tables=[]
        for r in cur.fetchall():
            cur.execute("""
              SELECT column_name as name, data_type as type
              FROM information_schema.columns
              WHERE table_schema=%s AND table_name=%s
              ORDER BY ordinal_position
            """,(r["schema"], r["name"]))
            cols = cur.fetchall()
            cur.execute(f'SELECT * FROM "{r["schema"]}"."{r["name"]}" LIMIT 5')
            rows = cur.fetchall()
            samples={}
            if rows:
                keys = rows[0].keys()
                for k in keys:
                    samples[k]=[x[k] for x in rows]
            tables.append({"schema":r["schema"], "name":r["name"], "columns":cols, "samples":samples})
        return {"SchemaCard":{"tables":tables}}

# -------------------- Preview / Validate / Approve --------------------
@app.post("/preview")
def preview(body: SQLBody):
    if body.conn_id:
        rec = load_connection(body.conn_id)
        conn = build_connector(rec)
        rows = conn.preview(body.sql, limit=body.limit or 20)
        return {"rows": rows}
    # fallback to local
    with get_cp_conn(False) as c, c.cursor() as cur:
        cur.execute(f"WITH cte AS ({body.sql}) SELECT * FROM cte LIMIT %s",(body.limit or 20,))
        return {"rows": cur.fetchall()}

@app.post("/validate")
def validate(body: SQLBody):
    if body.conn_id:
        rec = load_connection(body.conn_id)
        conn = build_connector(rec)
        v = conn.validate(body.sql)
        # normalize fields
        out = {"explain": v}
        if "total_cost" in v: out["total_cost"]=v["total_cost"]
        if "est_rows" in v: out["est_rows"]=v["est_rows"]
        if "plan_text" in v: out["plan_text"]=v["plan_text"]
        return out
    # fallback to local
    with get_cp_conn(False) as c, c.cursor(row_factory=dict_row) as cur:
        cur.execute(f"EXPLAIN (FORMAT JSON) {body.sql}")
        plan = cur.fetchone()["QUERY PLAN"]
        node = plan[0]["Plan"]
        return {"explain_json": plan, "total_cost": node.get("Total Cost"), "est_rows": node.get("Plan Rows")}

@app.post("/approve")
def approve(body: SQLBody):
    # connection-scoped execute + audit into control-plane
    if body.conn_id:
        rec = load_connection(body.conn_id)
        conn = build_connector(rec)
        cols, rows = conn.execute_readonly(body.sql, limit=body.limit)
        result_limited = body.limit is not None
        # audit
        with get_cp_conn(True) as cp, cp.cursor() as cur:
            cur.execute("""
                INSERT INTO audit_events(user_question, sql_text, row_count, result_limited, approval_ts, conn_id, engine, database, schema)
                VALUES (%s,%s,%s,%s,now(),%s,%s,%s,%s)
                RETURNING id
            """,(body.question or "", body.sql, len(rows), result_limited, rec["id"], rec["driver"], None, None))
            aid = cur.fetchone()[0]
        return {"ok": True, "row_count": len(rows), "columns": cols, "rows": rows, "audit_id": aid}
    # fallback local
    with get_cp_conn(False) as c, c.cursor() as cur:
        cur.execute(body.sql)
        cols = [d[0] for d in cur.description] if cur.description else []
        rows = cur.fetchall() if cur.description else []
        with get_cp_conn(True) as cp, cp.cursor() as cur2:
            cur2.execute("""
                INSERT INTO audit_events(user_question, sql_text, row_count, result_limited, approval_ts)
                VALUES (%s,%s,%s,%s,now())
                RETURNING id
            """,(body.question or "", body.sql, len(rows), body.limit is not None))
            aid = cur2.fetchone()[0]
        return {"ok": True, "row_count": len(rows), "columns": cols, "rows": rows, "audit_id": aid}

# -------------------- existing dataset ingestion stays available --------------------
@app.post("/datasets/from-url")
def from_url(body: FromURL):
    # delegate to existing loader script via simple call (kept for backward-compat)
    return {"ok": True, "note": "URL ingestion retained; Person B/C may hide it in UI later."}
